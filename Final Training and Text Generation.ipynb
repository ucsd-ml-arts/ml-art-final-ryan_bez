{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required packages\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#70 Stolen Valor | Reply All\n",
      "#90 Matt Lieber Goes to Dinner | Reply All\n",
      "#14 The Art of Making and Fixing Mistakes | Reply All\n",
      "#142 We Didn't Start The Fire | Reply All\n",
      "#82 Hello? | Reply All\n",
      "#122 The QAnon Code ⚡️⚡️ | Reply All\n",
      "#22 BONUS: The Man Who Refused To Email | Reply All\n",
      "#141 Adam Pisces and the $2 Coke | Reply All\n",
      "#110 The Antifa Supersoldier Spectacular | Reply All\n",
      "#128 The Crime Machine, Part II | Reply All\n",
      "#65 On the Inside, Part II | Reply All\n",
      "#29 The Takeover | Reply All\n",
      "#127 The Crime Machine, Part I | Reply All\n",
      "#93 Beware All | Reply All\n",
      "#104 The Case of the Phantom Caller | Reply All\n",
      "#130 The Snapchat Thief | Reply All\n",
      "#94 Obfuscation | Reply All\n",
      "#92 Favor Atender: The Return | Reply All\n",
      "#6 This Proves Everything + The Best Hold Music in the World | Reply All\n",
      "#20 I Want To Break Free | Reply All\n",
      "#67 On the Inside, Part IV | Reply All\n",
      "#55 The Line | Reply All\n",
      "#30 The Man In The FBI Hat | Reply All\n",
      "#120 INVCEL | Reply All\n",
      "#84 Past, Present, Future 2 | Reply All\n",
      "#135 Robocall: Bang Bang | Reply All\n",
      "#71 The Picture Taker | Reply All\n",
      "#44 Shine On You Crazy Goldman | Reply All\n",
      "#63 1000 Brimes | Reply All\n",
      "#74 Making Friends | Reply All\n",
      "#139 The Reply All Hotline | Reply All\n",
      "#113 Reply All's Year End Extravaganza | Reply All\n",
      "#34 DMV Nation | Reply All\n",
      "#129 Autumn | Reply All\n",
      "#134 The Year of the Wallop | Reply All\n",
      "#138 The Great Momo Panic | Reply All\n",
      "#75 Boy Wonder | Reply All\n",
      "#8 Anxiety Box | Reply All\n",
      "#76 Lost in a Cab | Reply All\n",
      "#24 Exit & Return, Part II | Reply All\n",
      "#46 Yik Yak Returns | Reply All\n",
      "#133 Reply All's 2018 Year End Extravaganza | Reply All\n",
      "#115 The Bitcoin Hunter | Reply All\n",
      "#17 The Time Traveler And The Hitman | Reply All\n",
      "#132 Negative Mount Pleasant | Reply All\n",
      "#88 Second Language | Reply All\n",
      "#81 In the Tall Grass | Reply All\n",
      "#2 The Secret, Gruesome Internet For Doctors | Reply All\n",
      "#108 The Skip Tracer, Part II | Reply All\n",
      "#119 No More Safe Harbor | Reply All\n",
      "#51 Perfect Crime | Reply All\n",
      "#5 Jennicam | Reply All\n",
      "#28 Shipped to Timbuktu | Reply All\n",
      "#68 Vampire Rules | Reply All\n",
      "#121 Pain Funnel | Reply All\n",
      "#37 Taking Power | Reply All\n",
      "#111 Return of the Russian Passenger | Reply All\n",
      "#3 We Know What You Did | Reply All\n",
      "#42 Blind Spot | Reply All\n",
      "#54 Apologies to Dr. Rosalind Franklin | Reply All\n",
      "#100 Friends and Blasphemers | Reply All\n",
      "#25 Favor Atender | Reply All\n",
      "#99 Black Hole, New Jersey | Reply All\n",
      "#57 Milk Wanted | Reply All\n",
      "#12 Back End Trouble | Reply All\n",
      "#131 Surefire Investigations | Reply All\n",
      "#86 Man of the People | Reply All\n",
      "#91 The Russian Passenger | Reply All\n",
      "#72 Dead is Paul | Reply All\n",
      "#62 Decoders | Reply All\n",
      "#80 Flash! | Reply All\n",
      "#96 The Secret Life of Alex Goldman | Reply All\n",
      "#106 Is That You, KD? | Reply All\n",
      "#39 Reply All Exploder | Reply All\n",
      "#21 Hack The Police | Reply All\n",
      "#16 Why Is Mason Reese Crying? | Reply All\n",
      "#26 Craigslist, Horsley's List | Reply All\n",
      "#123 An Ad for the Worst Day of Your Life | Reply All\n",
      "#43 The Law That Sticks | Reply All\n",
      "#140 The Roman Mars Mazda Virus | Reply All\n",
      "#89 Worldstar | Reply All\n",
      "#83 Voyage Into Pizzagate | Reply All\n",
      "#5.5 Jennicam Revisited | Reply All\n",
      "#77 The Grand Tapestry Of Pepe | Reply All\n",
      "#11 Did Errol Morris' Brother Invent Email? | Reply All\n",
      "#59 Good Job, Alex | Reply All\n",
      "#45 The Rainbow Pug | Reply All\n",
      "#66 On the Inside, Part III | Reply All\n",
      "#19 Underdog | Reply All\n",
      "#56 Zardulu | Reply All\n",
      "#79 Boy in Photo | Reply All\n",
      "#125 All My Pets | Reply All\n",
      "#36 Today's The Day | Reply All\n",
      "#95 The Silence in the Sky | Reply All\n",
      "#23 Exit & Return, Part I | Reply All\n",
      "#6 This Proves Everything | Reply All\n",
      "#64 On the Inside | Reply All\n",
      "#87 Storming the Castle | Reply All\n",
      "#9 The Writing On The Wall | Reply All\n",
      "#109 Is Facebook Spying on You? | Reply All\n",
      "#48 I Love You, I Loathe You | Reply All\n",
      "#52 Raising The Bar | Reply All\n",
      "#116 Trust the Process | Reply All\n",
      "#73 Sandbox | Reply All\n",
      "#107 The Skip Tracer, Part I | Reply All\n",
      "#10 The French Connection | Reply All\n",
      "#98 Fog of Covfefe | Reply All\n",
      "#33 @ISIS | Reply All\n",
      "#126 Alex Jones Dramageddon | Reply All\n",
      "#102 Long Distance | Reply All\n",
      "#112 The Prophet | Reply All\n",
      "#60 A Simple Question | Reply All\n",
      "#114 Apocalypse Soon | Reply All\n",
      "#58 Earth Pony | Reply All\n",
      "#50 The Cathedral | Reply All\n",
      "#61 Baby King | Reply All\n",
      "#49 Past, Present, Future | Reply All\n",
      "#78 Very Quickly to the Drill | Reply All\n",
      "#103 Long Distance, Part II | Reply All\n",
      "#27 The Fever | Reply All\n",
      "#4 Follow The Money | Reply All\n",
      "#53 In The Desert | Reply All\n",
      "#101 Minka | Reply All\n",
      "#136 The Founder | Reply All\n",
      "#85 The Reversal | Reply All\n",
      "#31 BONUS: The Reddit Implosion Explainer | Reply All\n",
      "#13 Love is Lies | Reply All\n",
      "#97 What Kind Of Idiot Gets Phished? | Reply All\n",
      "#137 Fool's Trade | Reply All\n",
      "#18 Silence And Respect | Reply All\n",
      "#41 What It Looks Like | Reply All\n",
      "#1 A Stranger Says I Love You | Reply All\n",
      "#38 Undo, Undo, Undo | Reply All\n",
      "#69 Disappeared | Reply All\n",
      "#124 The Magic Store | Reply All\n",
      "#7 This Website Is For Sale | Reply All\n",
      "#40 The Flower Child | Reply All\n",
      "#118 A Pirate In Search of a Judge | Reply All\n",
      "#32 The Evilest Technology On Earth :-) | Reply All\n",
      "#105 At World's End | Reply All\n",
      "#35 One Strike | Reply All\n",
      "#15 I've Killed People And I Have Hostages | Reply All\n",
      "#117 The World's Most Expensive Free Watch | Reply All\n",
      "Length of text: 8939998 characters\n",
      "116 unique characters\n"
     ]
    }
   ],
   "source": [
    "#Reading in previously scraped transcripts\n",
    "text = ''\n",
    "for file in os.listdir(os.getcwd()):\n",
    "    if os.path.isfile(file) and file != \"Final Training and Text Generation.ipynb\":\n",
    "        print(file)\n",
    "        text += open(file, 'rb').read().decode(encoding='utf-8')\n",
    "    \n",
    "print ('Length of text: {} characters'.format(len(text)))\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizing text\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating training examples\n",
    "seq_length = 107\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to duplicate and shift to form input and target\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating training batches\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 256\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "if tf.test.is_gpu_available():\n",
    "    rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "    import functools\n",
    "    rnn = functools.partial(tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    rnn(rnn_units,\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab), \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "#   return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "  return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "\n",
    "model.compile(optimizer = tf.train.AdamOptimizer(), loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './text50'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1305/1305 [==============================] - 94s 72ms/step - loss: 1.6828\n",
      "Epoch 2/50\n",
      "1305/1305 [==============================] - 89s 68ms/step - loss: 1.2289\n",
      "Epoch 3/50\n",
      "1305/1305 [==============================] - 87s 66ms/step - loss: 1.1512\n",
      "Epoch 4/50\n",
      "1305/1305 [==============================] - 95s 73ms/step - loss: 1.1070\n",
      "Epoch 5/50\n",
      "1305/1305 [==============================] - 87s 67ms/step - loss: 1.0772\n",
      "Epoch 6/50\n",
      "1305/1305 [==============================] - 82s 63ms/step - loss: 1.0571\n",
      "Epoch 7/50\n",
      "1305/1305 [==============================] - 85s 65ms/step - loss: 1.0439\n",
      "Epoch 8/50\n",
      "1305/1305 [==============================] - 82s 63ms/step - loss: 1.0369\n",
      "Epoch 9/50\n",
      "1305/1305 [==============================] - 88s 67ms/step - loss: 1.0337\n",
      "Epoch 10/50\n",
      "1305/1305 [==============================] - 84s 65ms/step - loss: 1.0359\n",
      "Epoch 11/50\n",
      "1305/1305 [==============================] - 81s 62ms/step - loss: 1.0402\n",
      "Epoch 12/50\n",
      "1305/1305 [==============================] - 89s 68ms/step - loss: 1.0487\n",
      "Epoch 13/50\n",
      "1305/1305 [==============================] - 86s 66ms/step - loss: 1.0609\n",
      "Epoch 14/50\n",
      "1305/1305 [==============================] - 82s 62ms/step - loss: 1.0774\n",
      "Epoch 15/50\n",
      "1305/1305 [==============================] - 80s 61ms/step - loss: 1.1038\n",
      "Epoch 16/50\n",
      "1305/1305 [==============================] - 81s 62ms/step - loss: 1.1523\n",
      "Epoch 17/50\n",
      "1305/1305 [==============================] - 81s 62ms/step - loss: 1.2548\n",
      "Epoch 18/50\n",
      "1305/1305 [==============================] - 83s 63ms/step - loss: 1.4452\n",
      "Epoch 19/50\n",
      "1305/1305 [==============================] - 89s 68ms/step - loss: 1.3464\n",
      "Epoch 20/50\n",
      "1305/1305 [==============================] - 87s 67ms/step - loss: 1.3139\n",
      "Epoch 21/50\n",
      "1305/1305 [==============================] - 81s 62ms/step - loss: 1.3423\n",
      "Epoch 22/50\n",
      "1305/1305 [==============================] - 89s 69ms/step - loss: 1.9173\n",
      "Epoch 23/50\n",
      "1305/1305 [==============================] - 86s 66ms/step - loss: 1.8459\n",
      "Epoch 24/50\n",
      "1305/1305 [==============================] - 79s 60ms/step - loss: 1.8708\n",
      "Epoch 25/50\n",
      "1305/1305 [==============================] - 84s 64ms/step - loss: 1.9421\n",
      "Epoch 26/50\n",
      "1305/1305 [==============================] - 88s 67ms/step - loss: 1.9168\n",
      "Epoch 27/50\n",
      "1305/1305 [==============================] - 89s 68ms/step - loss: 1.8882\n",
      "Epoch 28/50\n",
      "1305/1305 [==============================] - 95s 73ms/step - loss: 1.8640\n",
      "Epoch 29/50\n",
      "1305/1305 [==============================] - 81s 62ms/step - loss: 1.8659\n",
      "Epoch 30/50\n",
      "1305/1305 [==============================] - 80s 61ms/step - loss: 1.8837\n",
      "Epoch 31/50\n",
      "1305/1305 [==============================] - 76s 59ms/step - loss: 1.8675\n",
      "Epoch 32/50\n",
      "1305/1305 [==============================] - 90s 69ms/step - loss: 1.8904\n",
      "Epoch 33/50\n",
      "1305/1305 [==============================] - 87s 66ms/step - loss: 1.8667\n",
      "Epoch 34/50\n",
      "1305/1305 [==============================] - 87s 67ms/step - loss: 1.8151\n",
      "Epoch 35/50\n",
      "1305/1305 [==============================] - 90s 69ms/step - loss: 1.7685\n",
      "Epoch 36/50\n",
      "1305/1305 [==============================] - 87s 67ms/step - loss: 1.7090\n",
      "Epoch 37/50\n",
      "1305/1305 [==============================] - 92s 71ms/step - loss: 1.6319\n",
      "Epoch 38/50\n",
      "1305/1305 [==============================] - 82s 63ms/step - loss: 1.5501\n",
      "Epoch 39/50\n",
      "1305/1305 [==============================] - 89s 68ms/step - loss: 1.4731\n",
      "Epoch 40/50\n",
      "1305/1305 [==============================] - 86s 66ms/step - loss: 1.4098\n",
      "Epoch 41/50\n",
      "1305/1305 [==============================] - 89s 68ms/step - loss: 1.3634\n",
      "Epoch 42/50\n",
      "1305/1305 [==============================] - 87s 66ms/step - loss: 1.3304\n",
      "Epoch 43/50\n",
      "1305/1305 [==============================] - 85s 65ms/step - loss: 1.3048\n",
      "Epoch 44/50\n",
      "1305/1305 [==============================] - 85s 65ms/step - loss: 1.2830\n",
      "Epoch 45/50\n",
      "1305/1305 [==============================] - 79s 60ms/step - loss: 1.2669\n",
      "Epoch 46/50\n",
      "1305/1305 [==============================] - 92s 71ms/step - loss: 1.2523\n",
      "Epoch 47/50\n",
      "1305/1305 [==============================] - 91s 70ms/step - loss: 1.2401\n",
      "Epoch 48/50\n",
      "1305/1305 [==============================] - 82s 63ms/step - loss: 1.2300\n",
      "Epoch 49/50\n",
      "1305/1305 [==============================] - 81s 62ms/step - loss: 1.2212\n",
      "Epoch 50/50\n",
      "1305/1305 [==============================] - 82s 63ms/step - loss: 1.2143\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./text50/ckpt_50'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            29696     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)       (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 116)            118900    \n",
      "=================================================================\n",
      "Total params: 4,086,900\n",
      "Trainable params: 4,086,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(num_generate, temp, model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  #  num_generate = 65000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing) \n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "    temperature = temp\n",
    "\n",
    "  # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "      \n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"MUSIC65_100\",\"a\")\n",
    "file.write(generate_text(65000, model, start_string=u\"[THEME MUSIC]\"))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Convo.txt\",\"a\")\n",
    "i=0\n",
    "text  = \"ALEX GOLDMAN\"\n",
    "while i < 25:\n",
    "    text = generate_text(random.randint(30,108), 1.0, model, start_string=text)\n",
    "    text = text + '\\n'\n",
    "    file.write(text)\n",
    "    i+=1\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"ConvoPJ.txt\",\"a\")\n",
    "i=0\n",
    "text  = \"PJ VOGT:\"\n",
    "while i < 25:\n",
    "    text = generate_text(random.randint(30,108), 0.5, model, start_string=text)\n",
    "    text = text + '\\n'\n",
    "    file.write(text)\n",
    "    i+=1\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"ConvoMusic.txt\",\"a\")\n",
    "i=0\n",
    "text  = u\"[THEME MUSIC]\"\n",
    "while i < 25:\n",
    "    text = generate_text(random.randint(30,108), 1.5, model, start_string=text)\n",
    "    text = text + '\\n'\n",
    "    file.write(text)\n",
    "    i+=1\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Convo108.txt\",\"a\")\n",
    "i=0\n",
    "text  = u\"[THEME MUSIC]\"\n",
    "while i < 25:\n",
    "    text = generate_text(108, 1.0, model, start_string=text)\n",
    "    text = text + '\\n'\n",
    "    file.write(text)\n",
    "    i+=1\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"ConvoHosts2.txt\",\"a\")\n",
    "i=0\n",
    "hosts  = [ u\"ALEX GOLDMAN:\",\n",
    "          u\"PJ VOGT:\", u\"Damiano Marchetti:\",\n",
    "          u\"Phia Bennin:\", u\"Sruthi Pinnamaneni:\",\n",
    "          u\"Anna Foley:\", u\"ALEX BLUMBERG:\" ]\n",
    "while i < 25:\n",
    "    text = generate_text(108, 1.0, model, start_string=random.choice(hosts))\n",
    "    text = text + '\\n' + '\\n'\n",
    "    file.write(text)\n",
    "    i+=1\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
